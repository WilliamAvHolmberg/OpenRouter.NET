OPENROUTER.NET SDK - COMPREHENSIVE USAGE GUIDE
================================================

This document provides a complete reference for using the OpenRouter.NET SDK, including all patterns,
configuration options, and advanced features.

VERSION: 0.1.0
REQUIRES: .NET 9.0 or higher
NAMESPACE ROOT: OpenRouter.NET

================================================================================
0. GETTING STARTED - REQUIRED USING STATEMENTS
================================================================================

⚠️ CRITICAL: These using statements are REQUIRED for the SDK to work correctly.
Missing these will cause compilation errors!

BASIC FUNCTIONALITY (always required):
---
using OpenRouter.NET;          // Core client
using OpenRouter.NET.Models;   // Request/response models
---

TOOL CALLING (required if using tools):
---
using OpenRouter.NET.Tools;    // [ToolMethod], [ToolParameter] attributes
                               // AND RegisterTool extension methods
---

STREAMING (optional, for streaming extensions):
---
using OpenRouter.NET.Streaming; // StreamChunk and related types
---

ARTIFACT SUPPORT (optional, for artifact helpers):
---
using OpenRouter.NET.Artifacts; // Artifact definitions and helpers
---

COMMON ERROR: Missing using OpenRouter.NET.Tools
------------------------------------------------
If you see this error:
  "No argument given that corresponds to required parameter 'description'"

Solution: Add `using OpenRouter.NET.Tools;`

Without this namespace, C# can't see the RegisterTool extension methods and
falls back to the wrong overload!

================================================================================
1. CORE CLIENT USAGE PATTERNS
================================================================================

1.1 CONFIGURATION
-----------------

The SDK requires an API key from OpenRouter (https://openrouter.ai/keys).

OPTION A: appsettings.json (recommended for ASP.NET Core):
---
// appsettings.json
{
  "OpenRouter": {
    "ApiKey": "sk-or-v1-...",
    "SiteUrl": "https://myapp.com",
    "SiteName": "My Application"
  }
}

// Program.cs
builder.Services.AddSingleton(sp =>
{
    var config = sp.GetRequiredService<IConfiguration>();
    return new OpenRouterClient(
        config["OpenRouter:ApiKey"]
        ?? throw new InvalidOperationException("OpenRouter:ApiKey not configured")
    );
});
---

OPTION B: Environment variable:
---
// Set in terminal
export OPENROUTER_API_KEY="sk-or-v1-..."

// Or in code
var client = new OpenRouterClient(
    Environment.GetEnvironmentVariable("OPENROUTER_API_KEY")
    ?? throw new InvalidOperationException("OPENROUTER_API_KEY not set")
);
---

OPTION C: Direct initialization (simple apps):
---
var client = new OpenRouterClient("sk-or-v1-...");
---

OPTION D: Full configuration with all options:
---
var options = new OpenRouterClientOptions
{
    ApiKey = "sk-or-v1-...",
    SiteUrl = "https://yourapp.com",           // Optional: Used for HTTP-Referer header
    SiteName = "My App",                        // Optional: Used for X-Title header
    BaseUrl = "https://openrouter.ai/api/v1",  // Default: OpenRouter production URL
    HttpClient = new HttpClient(),              // Optional: Provide custom HttpClient
    OnLogMessage = message => Console.WriteLine(message)  // Optional: Logging callback
};

var client = new OpenRouterClient(options);
---

Key Configuration Properties:
- ApiKey (required): Your OpenRouter API key
- SiteUrl (optional): Your application's URL - sent to OpenRouter for tracking
- SiteName (optional): Your application's name - sent to OpenRouter for tracking
- BaseUrl: Can override for testing or custom endpoints (default: https://openrouter.ai/api/v1)
- HttpClient: Share an existing HttpClient or let SDK create one
- OnLogMessage: Callback for debug logging from the SDK

Important Notes:
- The SDK manages HttpClient disposal by default if not provided
- If you provide your own HttpClient, you're responsible for disposal
- Always dispose OpenRouterClient when done: client.Dispose()
- Thread-safe for concurrent requests

1.2 PUBLIC METHODS (NON-STREAMING)
-----------------------------------

CreateChatCompletionAsync(request, cancellationToken)
- Purpose: Send a non-streaming chat completion request
- Returns: Task<ChatCompletionResponse>
- Throws: OpenRouterException and subtypes on API errors
- Usage: For simple request-response patterns where streaming isn't needed
- Example:
  ---
  var response = await client.CreateChatCompletionAsync(
      new ChatCompletionRequest
      {
          Model = "openai/gpt-4",
          Messages = new List<Message>
          {
              Message.FromSystem("You are a helpful assistant."),
              Message.FromUser("Hello!")
          }
      }
  );
  var content = response.GetContent();
  ---

GetModelsAsync(cancellationToken)
- Purpose: Fetch list of available models from OpenRouter
- Returns: Task<List<ModelInfo>>
- Note: This is an expensive operation - cache results when possible
- Each ModelInfo contains: Id, Name, ContextLength, Architecture, etc.

GetLimitsAsync(cancellationToken)
- Purpose: Get your account limits and usage statistics
- Returns: Task<UserLimits>
- Contains: Rate limit info, remaining requests, quota usage

GetGenerationAsync(generationId, cancellationToken)
- Purpose: Retrieve metadata about a specific generation
- Returns: Task<GenerationInfo>
- Note: Requires the generation ID returned in responses

1.3 RESOURCE MANAGEMENT
-----------------------

Proper cleanup:
---
using var client = new OpenRouterClient(apiKey);
var response = await client.CreateChatCompletionAsync(request);
// Automatically disposed when exiting using block
---

Or explicit disposal:
---
var client = new OpenRouterClient(apiKey);
try
{
    var response = await client.CreateChatCompletionAsync(request);
}
finally
{
    client.Dispose();
}
---

================================================================================
1.5 COMMON ISSUES AND SOLUTIONS
================================================================================

ISSUE: "No argument given that corresponds to required parameter 'description'"
-------------------------------------------------------------------------------
Error when calling client.RegisterTool(calculator, nameof(calculator.Add))

CAUSE: Missing `using OpenRouter.NET.Tools;` namespace
SOLUTION: Add the using statement at the top of your file

---
using OpenRouter.NET.Tools;  // ← Required for RegisterTool extension method
---

ISSUE: "Authentication failed: No auth credentials found"
----------------------------------------------------------
CAUSE: API key not set or not found
SOLUTION:
- Check appsettings.json has correct key under "OpenRouter:ApiKey"
- Check environment variable OPENROUTER_API_KEY is set
- Verify API key is valid at https://openrouter.ai/keys

ISSUE: Tools registered in one request appear in subsequent requests
---------------------------------------------------------------------
CAUSE: OpenRouterClient registered as Singleton maintains tool state
SOLUTION: Either:
1. Clear tools before each request (if feature available)
2. Create scoped OpenRouterClient instances
3. This is expected behavior - tools persist on the client instance

ISSUE: Stream endpoints return partial data or don't flush
-----------------------------------------------------------
CAUSE: Not using Server-Sent Events (SSE) format correctly
SOLUTION: Set ContentType and flush after each write:

---
Response.ContentType = "text/event-stream";
await Response.WriteAsync($"data: {json}\n\n");
await Response.Body.FlushAsync();
---

ISSUE: Message.Content is null or wrong type
---------------------------------------------
CAUSE: Content can be string, List<ContentPart>, or null
SOLUTION: Use GetContent() extension method:

---
var content = response.GetContent();  // Returns string? safely
---

================================================================================
2. REQUEST/RESPONSE PATTERNS
================================================================================

2.1 CONSTRUCTING CHAT COMPLETION REQUESTS
-------------------------------------------

Basic structure:
---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",              // Required: Model identifier
    Messages = new List<Message>
    {
        Message.FromSystem("You are helpful."),
        Message.FromUser("What is 2+2?")
    },
    Temperature = 0.7f,                   // Optional: 0-2, default ~1
    MaxTokens = 1000,                     // Optional: Max completion tokens
    TopP = 0.9f,                          // Optional: Nucleus sampling
    TopK = 40,                            // Optional: Top-k sampling
    Stream = true                         // Optional: Enable streaming (default for StreamAsync)
};
---

Core Properties:
- Model (string): Model identifier like "openai/gpt-4o", "anthropic/claude-3.5-sonnet"
- Messages (List<Message>): Conversation history, order matters
- Temperature (float?): Randomness 0=deterministic, 2=creative
- MaxTokens (int?): Limit on completion length
- TopP (float?): Nucleus sampling parameter
- TopK (int?): Limit to top K most likely tokens
- FrequencyPenalty (float?): Penalize frequently used tokens
- PresencePenalty (float?): Penalize repeated content
- RepetitionPenalty (float?): Additional repetition control
- Seed (int?): For reproducible outputs
- Stop (object?): Stop sequence(s) - string or array of strings
- ResponseFormat (ResponseFormat?): Set type="json_object" for JSON mode
- Tools (List<Tool>?): Available tools (auto-added if registered with client)
- ToolChoice (object?): Control tool calling - "auto", "none", or specific tool
- Provider (object?): Route to specific provider
- Route (string?): Routing preference
- Models (List<string>?): Fallback model list for load balancing
- Transforms (List<string>?): Apply transformations
- LogitBias (Dictionary<int, float>?): Adjust token probabilities
- TopLogprobs (int?): Include token probabilities
- MinP (float?): Minimum probability sampling
- TopA (float?): Top-a sampling threshold
- Prediction (Prediction?): Prefix for faster completion
- ToolLoopConfig (ToolLoopConfig?): Configure automatic tool calling

2.2 MESSAGE CONSTRUCTION HELPERS
---------------------------------

Static factory methods on Message class:

User message (text):
---
var msg = Message.FromUser("What is AI?");
---

User message (multimodal with images):
---
var msg = Message.FromUser(new List<ContentPart>
{
    new TextContent("What's in this image?"),
    new ImageContent("https://example.com/image.jpg", detail: "high")
});
---

System message:
---
var msg = Message.FromSystem("You are a Python expert. Format all code in markdown blocks.");
---

Assistant message:
---
var msg = Message.FromAssistant("Here's my response");
---

Tool result message:
---
var msg = Message.FromTool("{"result": "success"}", toolCallId: "call_123");
---

Direct construction:
---
var msg = new Message
{
    Role = "user",
    Content = "Hello",
    Name = "UserName"  // Optional: identifies this speaker
};
---

Multimodal content types:

TextContent:
- Properties: Text (string)
- Type property returns "text"

ImageContent:
- Properties: ImageUrl (object with Url and Detail)
- Detail values: "auto", "low", "high"
- URL can be: https://, data:image/base64, or file path
- Type property returns "image_url"

Important notes:
- Message.Content can be string, List<ContentPart>, or null
- Tool messages (Role="tool") must include ToolCallId
- Messages are processed in order in the list
- Always include system message as first message for best results

2.3 RESPONSE NAVIGATION AND EXTRACTION
---------------------------------------

ChatCompletionResponse structure:
- Id (string): Unique response identifier
- Model (string): Model that generated response
- Object (string): Always "chat.completion"
- Created (long): Unix timestamp
- SystemFingerprint (string?): Fingerprint for reproducibility
- Choices (List<Choice>): Array of completions
- Usage (ResponseUsage?): Token counts

Accessing response content:

Get first completion:
---
var response = await client.CreateChatCompletionAsync(request);
var content = response.Choices?[0].Message?.Content?.ToString();
---

Using extension method:
---
var content = response.GetContent();  // string?
var message = response.GetMessage();  // Message?
var reason = response.GetFinishReason();  // string?
---

Check for tool calls:
---
if (response.HasToolCalls())
{
    var toolCalls = response.GetToolCalls();
    foreach (var call in toolCalls)
    {
        var toolName = call.Function?.Name;
        var arguments = call.Function?.Arguments;  // JSON string
    }
}
---

Access token usage:
---
var usage = response.Usage;
var totalTokens = usage?.TotalTokens;  // Total tokens used
var promptTokens = usage?.PromptTokens;  // Input tokens
var completionTokens = usage?.CompletionTokens;  // Output tokens
---

Choice properties:
- Index (int): Position in choices array
- Message (Message?): The response message
- FinishReason (string?): "stop", "length", "tool_calls", etc.

2.4 ERROR HANDLING & EXCEPTION TYPES
-------------------------------------

Exception hierarchy:

OpenRouterException (base)
├─ OpenRouterAuthException (401)
├─ OpenRouterRateLimitException (429)
├─ OpenRouterBadRequestException (400)
├─ OpenRouterModelNotFoundException (404)
└─ OpenRouterServerException (500+)

Handling specific errors:

Rate limiting:
---
try
{
    var response = await client.CreateChatCompletionAsync(request);
}
catch (OpenRouterRateLimitException ex)
{
    var retryAfter = ex.RetryAfterSeconds;  // seconds to wait
    await Task.Delay(TimeSpan.FromSeconds(retryAfter ?? 60));
    // Retry request
}
---

Authentication:
---
catch (OpenRouterAuthException ex)
{
    Console.WriteLine("Invalid API key: " + ex.Message);
}
---

Model not found:
---
catch (OpenRouterModelNotFoundException ex)
{
    Console.WriteLine($"Model {ex.ModelId} not available");
}
---

Generic error handling:
---
catch (OpenRouterException ex)
{
    var statusCode = ex.StatusCode;  // HTTP status code
    var errorCode = ex.ErrorCode;    // Error code if provided
    Console.WriteLine($"API Error: {ex.Message}");
}
---

Exception properties:
- Message (string): Error description
- StatusCode (int?): HTTP status code
- ErrorCode (string?): Machine-readable error code

Common error codes:
- "authentication_error": Invalid API key
- "rate_limit_exceeded": Too many requests
- "model_not_found": Model not available
- "bad_request": Invalid request format
- "server_error": OpenRouter server issue

================================================================================
3. STREAMING PATTERNS
================================================================================

3.1 BASIC STREAMING
-------------------

StreamAsync returns IAsyncEnumerable<StreamChunk>:

---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message> { Message.FromUser("Hello") }
};

await foreach (var chunk in client.StreamAsync(request))
{
    if (chunk.TextDelta != null)
    {
        Console.Write(chunk.TextDelta);  // Print streaming text
    }
    
    if (chunk.Completion != null)
    {
        Console.WriteLine($"Finished: {chunk.Completion.FinishReason}");
    }
}
---

StreamChunk record properties:

- IsFirstChunk (bool): True for first data chunk received
- ElapsedTime (TimeSpan): Elapsed since first data chunk (useful for TTFT)
- ChunkIndex (int): Sequential chunk number
- TextDelta (string?): Text content increment
- ToolCallDelta (ToolCall?): Tool call delta from model
- Artifact (ArtifactEvent?): Artifact started/progress/completed events
- ServerTool (ServerToolCall?): Server-side tool execution status
- ClientTool (ClientToolCall?): Client-side tool call pending
- Completion (CompletionMetadata?): Final completion info with finish reason
- Raw (ChatCompletionStreamResponse): Raw API response

CompletionMetadata:
- FinishReason (string?): "stop", "length", "tool_calls", etc.
- Model (string?): Model used
- Id (string?): Response ID
- Usage (ResponseUsage?): Token counts (usually null in stream)

3.2 TIME TO FIRST TOKEN (TTFT) TRACKING
-----------------------------------------

TTFT is the time from request start to first content chunk:

---
TimeSpan? ttft = null;
await foreach (var chunk in client.StreamAsync(request))
{
    if (chunk.IsFirstChunk && chunk.TextDelta != null)
    {
        ttft = chunk.ElapsedTime;
        Console.WriteLine($"TTFT: {ttft.Value.TotalMilliseconds:F0}ms");
    }
    
    if (chunk.TextDelta != null)
    {
        // Streaming text
    }
}
---

Key timing properties:
- ElapsedTime: Time since streaming started
- IsFirstChunk: Indicator of first data chunk
- Time increases monotonically across chunks
- Use for performance monitoring and optimization

3.3 STREAMING EXTENSION METHODS
--------------------------------

StreamTextAsync (simple text collection):
---
var request = new ChatCompletionRequest { /* ... */ };
var fullText = new System.Text.StringBuilder();

await client.StreamTextAsync(request, 
    onText: text => fullText.Append(text));

Console.WriteLine(fullText.ToString());
---

StreamTextAsync (with callbacks):
---
await client.StreamTextAsync(request,
    onText: text => Console.Write(text),
    onFirstChunk: chunk => Console.WriteLine("Started streaming"),
    onComplete: completion => Console.WriteLine($"Done: {completion.FinishReason}"));
---

StreamTextChunksAsync (async enumerable):
---
await foreach (var textChunk in client.StreamTextChunksAsync(request))
{
    Console.Write(textChunk);
}
---

CollectArtifactsAsync (collect artifacts from stream):
---
var artifacts = await client.CollectArtifactsAsync(request,
    onText: text => Console.Write(text));

foreach (var artifact in artifacts)
{
    Console.WriteLine($"Artifact: {artifact.Title} ({artifact.Type})");
    Console.WriteLine(artifact.Content);
}
---

StreamAndAccumulateAsync (collect complete message):
---
var message = await client.StreamAndAccumulateAsync(request,
    onChunk: chunk =>
    {
        if (chunk.TextDelta != null)
            Console.Write(chunk.TextDelta);
    });

Console.WriteLine($"Full content: {message.Content}");
if (message.ToolCalls != null)
{
    Console.WriteLine($"Tool calls: {message.ToolCalls.Length}");
}
---

3.4 STREAMING WITH CANCELLATION
--------------------------------

All streaming methods support CancellationToken:

---
var cts = new CancellationTokenSource();

var task = Task.Run(async () =>
{
    await foreach (var chunk in client.StreamAsync(request, cts.Token))
    {
        if (chunk.TextDelta != null)
            Console.Write(chunk.TextDelta);
    }
});

// Cancel after 5 seconds
await Task.Delay(5000);
cts.Cancel();

try
{
    await task;
}
catch (OperationCanceledException)
{
    Console.WriteLine("Streaming cancelled");
}
---

================================================================================
4. TOOL CALLING PATTERNS
================================================================================

4.1 SERVER-SIDE TOOLS - QUICK START (RECOMMENDED)
--------------------------------------------------

Tools allow the AI model to call functions in your code automatically.

⚠️ STEP 1: Add the required using statement (CRITICAL!)
---
using OpenRouter.NET.Tools;  // ← Without this, RegisterTool won't work!
---

STEP 2: Create your tool class with attributes
---
public class CalculatorTools
{
    [ToolMethod("Add two numbers")]
    public int Add(
        [ToolParameter("First number")] int a,
        [ToolParameter("Second number")] int b)
    {
        return a + b;
    }

    [ToolMethod("Subtract two numbers")]
    public int Subtract(
        [ToolParameter("First number")] int a,
        [ToolParameter("Second number")] int b)
    {
        return a - b;
    }

    [ToolMethod("Multiply two numbers")]
    public int Multiply(
        [ToolParameter("First number")] int a,
        [ToolParameter("Second number")] int b)
    {
        return a * b;
    }

    [ToolMethod("Divide two numbers")]
    public double Divide(
        [ToolParameter("Numerator")] double a,
        [ToolParameter("Denominator")] double b)
    {
        if (b == 0)
            throw new ArgumentException("Cannot divide by zero");
        return a / b;
    }
}
---

STEP 3: Register the tools with the client
---
var calculator = new CalculatorTools();
client.RegisterTool(calculator, nameof(calculator.Add));
client.RegisterTool(calculator, nameof(calculator.Subtract));
client.RegisterTool(calculator, nameof(calculator.Multiply));
client.RegisterTool(calculator, nameof(calculator.Divide));

// Can also chain:
client
    .RegisterTool(calculator, nameof(calculator.Add))
    .RegisterTool(calculator, nameof(calculator.Subtract))
    .RegisterTool(calculator, nameof(calculator.Multiply))
    .RegisterTool(calculator, nameof(calculator.Divide));
---

STEP 4: Use in requests (tools automatically included)
---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message> { Message.FromUser("What's 15 + 27?") }
    // Tools automatically included - no manual configuration needed!
};

await foreach (var chunk in client.StreamAsync(request))
{
    if (chunk.ServerTool != null)
    {
        Console.WriteLine($"Tool: {chunk.ServerTool.ToolName}");
        Console.WriteLine($"State: {chunk.ServerTool.State}");
        if (chunk.ServerTool.State == ToolCallState.Completed)
            Console.WriteLine($"Result: {chunk.ServerTool.Result}");
    }

    if (chunk.TextDelta != null)
    {
        Console.Write(chunk.TextDelta);
    }
}
---

That's it! The model can now call your calculator methods automatically.

4.2 ALTERNATIVE TOOL REGISTRATION PATTERNS
-------------------------------------------

Attribute-based registration (from 4.1 above) is RECOMMENDED, but here are alternatives:

---
public class CalculatorTools
{
    [ToolMethod("Add two numbers")]
    public int Add(
        [ToolParameter("First number")] int a,
        [ToolParameter("Second number")] int b)
    {
        return a + b;
    }

    [ToolMethod("multiply", "Multiply two numbers")]
    public int Multiply(
        [ToolParameter("First number")] int a,
        [ToolParameter("Second number")] int b)
    {
        return a * b;
    }

    [ToolMethod("Divide two numbers")]
    public double Divide(
        [ToolParameter("Numerator")] double a,
        [ToolParameter("Denominator")] double b)
    {
        if (b == 0)
            throw new ArgumentException("Cannot divide by zero");
        return a / b;
    }
}

// Register by method name (simplest when using attributes)
var calculator = new CalculatorTools();
client.RegisterTool(calculator, nameof(calculator.Add));
client.RegisterTool(calculator, nameof(calculator.Multiply));
client.RegisterTool(calculator, nameof(calculator.Divide));

// Can also use string literals
client.RegisterTool(calculator, "Add");
---

Generic registration with lambdas (when you need explicit control):

---
// For Func<T1, T2, TResult> - two parameters
client.RegisterTool<int, int, int>((a, b) => calculator.Add(a, b));
client.RegisterTool<double, double, double>((a, b) => calculator.Divide(a, b));

// For Func<T1, TResult> - single parameter
client.RegisterTool<string, string>((query) => SearchDatabase(query));

// For Func<TResult> - no parameters
client.RegisterTool<DateTime>(() => DateTime.UtcNow);

// IMPORTANT: The generic types are Func<...parameters..., TResult>
// NOT the individual parameter types!
// WRONG: client.RegisterTool<int, int, int>(calculator.Add);  ← This doesn't work!
// RIGHT: client.RegisterTool<int, int, int>((a, b) => calculator.Add(a, b));
---

COMMON MISTAKE: Using method references with generic RegisterTool

---
// ❌ WRONG - compiler error "cannot be used with type arguments"
client.RegisterTool<int, int, int>(calculator.Add);

// ✅ CORRECT - use lambda wrapper
client.RegisterTool<int, int, int>((a, b) => calculator.Add(a, b));

// ✅ BETTER - use method name (simplest with attributes)
client.RegisterTool(calculator, nameof(calculator.Add));
---

Manual registration (if no attributes):

---
client.RegisterTool(
    name: "get_weather",
    implementation: args => GetWeatherData(JsonDocument.Parse(args)),
    description: "Get current weather for a location",
    parameters: new
    {
        type = "object",
        properties = new
        {
            location = new { type = "string", description = "City name" },
            unit = new { type = "string", @enum = new[] { "C", "F" } }
        },
        required = new[] { "location" }
    },
    mode: ToolMode.AutoExecute);
---

Tool registration returns the client for chaining:

---
client
    .RegisterTool(calculator, "Add")
    .RegisterTool(calculator, "Multiply")
    .RegisterTool(calculator, "Divide");
---

**Using StreamAsync with Tools**

For AUTO-EXECUTE tools with streaming, use StreamAsync with tool execution visible in chunks:

---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message> { Message.FromUser("Add 5 and 3") }
    // Tools are automatically included if registered
};

await foreach (var chunk in client.StreamAsync(request))
{
    if (chunk.ServerTool?.State == ToolCallState.Executing)
    {
        Console.WriteLine($"Executing {chunk.ServerTool.ToolName}...");
    }

    if (chunk.ServerTool?.State == ToolCallState.Completed)
    {
        Console.WriteLine($"Result: {chunk.ServerTool.Result}");
    }

    if (chunk.ServerTool?.State == ToolCallState.Error)
    {
        Console.WriteLine($"Error: {chunk.ServerTool.Error}");
    }

    if (chunk.TextDelta != null)
    {
        Console.Write(chunk.TextDelta);
    }
}
---

4.2 CLIENT-SIDE TOOLS (MANUAL HANDLING)
----------------------------------------

Register tools that the client must handle:

---
client.RegisterClientTool(
    "fetch_user",
    "Fetch user data from database",
    new
    {
        type = "object",
        properties = new
        {
            userId = new { type = "string", description = "User ID" }
        },
        required = new[] { "userId" }
    });
---

Client handles execution:

---
await foreach (var chunk in client.StreamAsync(request))
{
    if (chunk.ClientTool != null)
    {
        var toolName = chunk.ClientTool.ToolName;
        var args = JsonDocument.Parse(chunk.ClientTool.Arguments);
        
        // Application handles the tool call
        var userId = args.RootElement.GetProperty("userId").GetString();
        var userData = await FetchUserFromDatabase(userId);
        
        // No automatic continuation - application decides next step
        Console.WriteLine($"Client tool: {toolName} awaiting handling");
    }
    
    if (chunk.TextDelta != null)
    {
        Console.Write(chunk.TextDelta);
    }
}
---

Key difference:
- AutoExecute: SDK calls tool, passes result back to model, continues conversation
- ClientSide: SDK yields control to application, waits for manual continuation
- ClientSide useful for: long-running operations, external APIs, async work

4.3 TOOL LOOP CONFIGURATION
-----------------------------

Control automatic tool-calling behavior:

---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message> { Message.FromUser("Do multiple calculations") },
    ToolLoopConfig = new ToolLoopConfig
    {
        Enabled = true,              // Enable automatic tool calling
        MaxIterations = 5,           // Max tool calls before stopping
        TimeoutPerCall = TimeSpan.FromSeconds(30)
    }
};

await foreach (var chunk in client.StreamAsync(request))
{
    // StreamAsync handles tool loops automatically
}
---

Disable tool loops:

---
request.ToolLoopConfig = new ToolLoopConfig { Enabled = false };
// Now tools only execute once per response
---

StreamChunk ServerToolCall structure:

- ToolName (string): Name of the tool being called
- ToolId (string): Unique identifier for this call
- Arguments (string): JSON arguments passed to tool
- State (ToolCallState): Executing, Completed, or Error
- Result (string?): Result if Completed
- Error (string?): Error message if Error state
- ExecutionTime (TimeSpan?): How long tool took to run

4.4 COMPLETE WEB API EXAMPLE WITH TOOLS
----------------------------------------

ASP.NET Core controller with tool calling:

---
using Microsoft.AspNetCore.Mvc;
using OpenRouter.NET;
using OpenRouter.NET.Models;
using OpenRouter.NET.Tools;  // ← REQUIRED
using System.Text;

[ApiController]
[Route("api/[controller]")]
public class ChatController : ControllerBase
{
    private readonly OpenRouterClient _client;
    private readonly ILogger<ChatController> _logger;

    public ChatController(OpenRouterClient client, ILogger<ChatController> logger)
    {
        _client = client;
        _logger = logger;
    }

    [HttpPost("tools")]
    public async Task<ActionResult<string>> ChatWithTools([FromBody] ToolRequest request)
    {
        try
        {
            // Create and register tools
            var calculator = new CalculatorTools();
            _client.RegisterTool(calculator, nameof(calculator.Add));
            _client.RegisterTool(calculator, nameof(calculator.Multiply));
            _client.RegisterTool(calculator, nameof(calculator.Subtract));
            _client.RegisterTool(calculator, nameof(calculator.Divide));

            var chatRequest = new ChatCompletionRequest
            {
                Model = request.Model,
                Messages = new List<Message> { Message.FromUser(request.Message) }
            };

            var responseText = new StringBuilder();

            // Option 1: StreamAsync - handle chunks directly
            await foreach (var chunk in _client.StreamAsync(chatRequest))
            {
                if (chunk.ServerTool != null)
                {
                    _logger.LogInformation(
                        "Tool {ToolName} - State: {State}, Result: {Result}",
                        chunk.ServerTool.ToolName,
                        chunk.ServerTool.State,
                        chunk.ServerTool.Result
                    );
                }

                if (chunk.TextDelta != null)
                {
                    responseText.Append(chunk.TextDelta);
                }
            }

            return Ok(responseText.ToString());
        }
        catch (OpenRouterException ex)
        {
            _logger.LogError(ex, "OpenRouter API error");
            return StatusCode(ex.StatusCode ?? 500, new { error = ex.Message, code = ex.ErrorCode });
        }
    }
}

// Tool class definition
public class CalculatorTools
{
    [ToolMethod("Add two numbers")]
    public int Add(
        [ToolParameter("First number")] int a,
        [ToolParameter("Second number")] int b)
    {
        return a + b;
    }

    [ToolMethod("Multiply two numbers")]
    public int Multiply(
        [ToolParameter("First number")] int a,
        [ToolParameter("Second number")] int b)
    {
        return a * b;
    }

    [ToolMethod("Subtract two numbers")]
    public int Subtract(
        [ToolParameter("First number")] int a,
        [ToolParameter("Second number")] int b)
    {
        return a - b;
    }

    [ToolMethod("Divide two numbers")]
    public double Divide(
        [ToolParameter("Numerator")] double a,
        [ToolParameter("Denominator")] double b)
    {
        if (b == 0)
            throw new ArgumentException("Cannot divide by zero");
        return a / b;
    }
}
---

Dependency injection setup (Program.cs):
---
builder.Services.AddSingleton(sp =>
{
    var apiKey = builder.Configuration["OpenRouter:ApiKey"]
        ?? throw new InvalidOperationException("OpenRouter API key not configured");
    return new OpenRouterClient(apiKey);
});
---

4.5 STREAMING ENDPOINTS WITH SERVER-SENT EVENTS (SSE)
------------------------------------------------------

For streaming responses in web APIs, use Server-Sent Events format:

---
[HttpPost("stream")]
public async Task StreamChat([FromBody] ChatRequest request)
{
    // CRITICAL: Set ContentType to text/event-stream
    Response.ContentType = "text/event-stream";

    try
    {
        var chatRequest = new ChatCompletionRequest
        {
            Model = request.Model,
            Messages = new List<Message> { Message.FromUser(request.Message) }
        };

        await foreach (var chunk in _client.StreamAsync(chatRequest))
        {
            if (chunk.TextDelta != null)
            {
                // Format as SSE: "data: {json}\n\n"
                var json = JsonSerializer.Serialize(new
                {
                    content = chunk.TextDelta,
                    isComplete = false
                });
                await Response.WriteAsync($"data: {json}\n\n");
                await Response.Body.FlushAsync();  // ← CRITICAL: Flush after each write
            }

            if (chunk.Completion != null)
            {
                var json = JsonSerializer.Serialize(new
                {
                    content = "",
                    isComplete = true,
                    finishReason = chunk.Completion.FinishReason
                });
                await Response.WriteAsync($"data: {json}\n\n");
                await Response.Body.FlushAsync();
            }
        }
    }
    catch (OpenRouterException ex)
    {
        _logger.LogError(ex, "Streaming error");
        var errorJson = JsonSerializer.Serialize(new
        {
            error = ex.Message,
            code = ex.ErrorCode
        });
        await Response.WriteAsync($"data: {errorJson}\n\n");
    }
}
---

Client-side JavaScript for SSE:
---
const eventSource = new EventSource('/api/chat/stream', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ message: 'Hello!', model: 'openai/gpt-4o' })
});

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    if (data.isComplete) {
        console.log('Stream complete:', data.finishReason);
        eventSource.close();
    } else {
        console.log('Content:', data.content);
        // Append to UI
        document.getElementById('output').textContent += data.content;
    }
};

eventSource.onerror = (error) => {
    console.error('SSE error:', error);
    eventSource.close();
};
---

Or using fetch with streaming:
---
const response = await fetch('/api/chat/stream', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ message: 'Hello!', model: 'openai/gpt-4o' })
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    const lines = chunk.split('\n').filter(line => line.startsWith('data: '));

    for (const line of lines) {
        const json = line.substring(6); // Remove "data: " prefix
        const data = JSON.parse(json);
        console.log('Received:', data);
    }
}
---

SSE Format Requirements:
- ContentType must be "text/event-stream"
- Each message must be "data: {content}\n\n"
- Must flush after each write
- Client receives events in real-time

================================================================================
5. ARTIFACT SYSTEM
================================================================================

5.1 ENABLING ARTIFACT SUPPORT
------------------------------

Artifacts are self-contained code, documents, or data that the model can generate.

Basic artifact support:

---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message> { Message.FromUser("Create a React component") }
};

request.EnableArtifactSupport();
// Adds system message instructing model to wrap code in artifact tags

var response = await client.CreateChatCompletionAsync(request);
---

With custom instructions:

---
request.EnableArtifactSupport(
    customInstructions: "Only create artifacts for code that is over 100 lines");
---

Enable specific artifact types:

---
request.EnableCodeArtifacts("javascript", "typescript", "python");
// Enables code artifacts with specific language constraints
---

Document artifacts:

---
request.EnableDocumentArtifacts("markdown", "html");
---

Custom artifact definitions:

---
var codeArtifact = new GenericArtifact("code", title: "script.js")
    .WithLanguage("javascript")
    .WithInstruction("Use modern ES6+ syntax");

var dataArtifact = new GenericArtifact("data", title: "data.json")
    .WithLanguage("json")
    .WithOutputFormat("Valid JSON only");

request.EnableArtifacts(codeArtifact, dataArtifact);
---

Using Artifacts helper:

---
var artifacts = new[]
{
    Artifacts.Code(language: "python"),
    Artifacts.Document(format: "markdown"),
    Artifacts.Data(title: "config.json", format: "json"),
    Artifacts.Custom("diagram", title: "diagram.mermaid")
        .WithInstruction("Use mermaid.js syntax")
};

request.EnableArtifacts(artifacts);
---

5.2 ARTIFACT DEFINITIONS
------------------------

ArtifactDefinition base class:

- Type (string): "code", "document", "data", or custom type
- PreferredTitle (string?): Suggested title
- Language (string?): Language or format for content
- Instruction (string?): Additional instructions for this artifact type
- OutputFormat (string?): Expected format constraints
- CustomAttributes (Dictionary?): Additional XML attributes

GenericArtifact fluent API:

---
new GenericArtifact("whiteboard", "diagram.drawio")
    .WithLanguage("drawio")
    .WithInstruction("Create a detailed architecture diagram")
    .WithOutputFormat("XML format for draw.io")
    .WithAttribute("theme", "light")
    .WithAttribute("exportable", "true")
---

5.3 ARTIFACT EVENTS IN STREAMS
-------------------------------

Artifact parsing happens automatically during streaming:

---
await foreach (var chunk in client.StreamAsync(request))
{
    if (chunk.Artifact is ArtifactStarted started)
    {
        Console.WriteLine($"Artifact started: {started.Title}");
        Console.WriteLine($"Type: {started.Type}");
        Console.WriteLine($"Language: {started.Language}");
        Console.WriteLine($"ID: {started.ArtifactId}");
    }
    
    if (chunk.Artifact is ArtifactContent content)
    {
        // ContentDelta contains incremental artifact content
        artifactBuilder.Append(content.ContentDelta);
    }
    
    if (chunk.Artifact is ArtifactCompleted completed)
    {
        Console.WriteLine($"Artifact complete: {completed.Title}");
        Console.WriteLine($"Full content:\n{completed.Content}");
        var artifact = new Artifact
        {
            Id = completed.ArtifactId,
            Type = completed.Type,
            Title = completed.Title,
            Content = completed.Content,
            Language = completed.Language
        };
    }
    
    if (chunk.TextDelta != null)
    {
        Console.Write(chunk.TextDelta);  // Text outside artifacts
    }
}
---

ArtifactEvent hierarchy:

ArtifactStarted:
- ArtifactId: Unique ID for this artifact
- Type: Artifact type
- Title: Display title
- Language: Language/format if applicable

ArtifactContent:
- ArtifactId: References the artifact
- Type: Artifact type
- ContentDelta: Incremental content chunk

ArtifactCompleted:
- ArtifactId: ID of completed artifact
- Type: Artifact type
- Title: Final title
- Content: Complete artifact content
- Language: Language/format

Artifact class:
- Id: Unique identifier
- Type: Artifact type
- Title: Display title
- Content: Full content
- Language: Language/format
- Metadata: Additional properties (Dictionary)
- CreatedAt: When artifact was created

5.4 EXTRACTING ARTIFACTS FROM RESPONSES
---------------------------------------

Use CollectArtifactsAsync extension:

---
var artifacts = await client.CollectArtifactsAsync(request,
    onText: text => Console.Write(text));

foreach (var artifact in artifacts)
{
    // Save artifact to file
    await File.WriteAllTextAsync(
        $"{artifact.Type}_{artifact.Title}",
        artifact.Content);
}
---

Manual collection:

---
var artifacts = new List<Artifact>();

await foreach (var chunk in client.StreamAsync(request))
{
    if (chunk.Artifact is ArtifactCompleted completed)
    {
        artifacts.Add(new Artifact
        {
            Id = completed.ArtifactId,
            Type = completed.Type,
            Title = completed.Title,
            Content = completed.Content,
            Language = completed.Language
        });
    }
}
---

Getting text without artifacts:

---
var textOnly = new StringBuilder();

await foreach (var chunk in client.StreamAsync(request))
{
    if (chunk.TextDelta != null)
    {
        textOnly.Append(chunk.TextDelta);
    }
}

// textOnly contains conversation text, not artifact content
---

================================================================================
6. CONVERSATION MANAGEMENT
================================================================================

6.1 MESSAGE HISTORY EXTENSIONS
-------------------------------

Adding messages to conversation:

---
var history = new List<Message>();

history.AddUserMessage("Hello, what can you do?");
history.AddAssistantMessage("I can help with various tasks...");
history.AddUserMessage("Can you write code?");
---

Clearing history:

---
// Keep system message, clear rest
history.ClearHistory(keepSystemPrompt: true);

// Clear everything
history.ClearHistory(keepSystemPrompt: false);
---

Getting summary:

---
var summary = history.GetConversationSummary();
// Returns formatted string with all messages and previews
Console.WriteLine(summary);
---

Getting message counts:

---
var totalMessages = history.GetMessageCount();      // All messages
var userMessages = history.GetMessageCount("user");  // Only user
var assistantMessages = history.GetMessageCount("assistant");
---

6.2 TOKEN AND COST ESTIMATION
------------------------------

Token estimation (rough approximation):

---
var estimatedTokens = history.EstimateTokenCount();
// Uses ~4 characters per token approximation
// Note: This is an estimate, actual tokens may vary
---

Cost estimation:

---
// Prices in USD per million tokens
decimal inputPrice = 0.015m;      // $0.015 per 1M input tokens
decimal outputPrice = 0.06m;      // $0.06 per 1M output tokens
int estimatedOutput = 500;        // Estimated output tokens

var estimatedCost = history.EstimateCost(inputPrice, outputPrice, estimatedOutput);
Console.WriteLine($"Estimated cost: ${estimatedCost:F6}");
---

Use after response:

---
var response = await client.CreateChatCompletionAsync(request);
var fullText = response.GetContent() ?? "";

// Add response to history
history.AddAssistantMessage(fullText);

// Calculate with actual output tokens
var actualOutputTokens = response.Usage?.CompletionTokens ?? 0;
var actualCost = history.EstimateCost(0.015m, 0.06m, actualOutputTokens);
Console.WriteLine($"Actual cost: ${actualCost:F6}");
---

6.3 MULTI-TURN CONVERSATIONS
-----------------------------

Complete conversation flow:

---
var client = new OpenRouterClient(apiKey);
var history = new List<Message>
{
    Message.FromSystem("You are a helpful assistant.")
};

history.AddUserMessage("What is machine learning?");

var response1 = await client.CreateChatCompletionAsync(new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = history
});

history.AddAssistantMessage(response1.GetContent() ?? "");

history.AddUserMessage("Can you give me an example?");

var response2 = await client.CreateChatCompletionAsync(new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = history
});

history.AddAssistantMessage(response2.GetContent() ?? "");

// History now contains entire conversation
Console.WriteLine(history.GetConversationSummary());
---

With streaming:

---
while (true)
{
    var userInput = Console.ReadLine();
    if (string.IsNullOrEmpty(userInput)) break;
    
    history.AddUserMessage(userInput);
    
    var responseText = new StringBuilder();
    var request = new ChatCompletionRequest
    {
        Model = "openai/gpt-4o",
        Messages = history
    };
    
    await foreach (var chunk in client.StreamAsync(request))
    {
        if (chunk.TextDelta != null)
        {
            responseText.Append(chunk.TextDelta);
            Console.Write(chunk.TextDelta);
        }
    }
    
    history.AddAssistantMessage(responseText.ToString());
    Console.WriteLine("\n");
}
---

6.4 CLIENT-SIDE HISTORY (PRODUCTION STATELESS PATTERN)
-------------------------------------------------------

For production-ready, horizontally-scalable applications, implement
client-side history management with ZERO server memory usage.

BENEFITS:
- ✅ Zero server memory - infinite scalability
- ✅ Survives server restarts - no data loss
- ✅ No session affinity - any server handles any request
- ✅ Natural limits - browser storage quota
- ✅ User privacy - data stays on device
- ✅ Works with tools and artifacts automatically

BACKEND PATTERN (ASP.NET Core):
---
app.MapPost("/api/stream-stateless", async (ChatRequest chatRequest, HttpContext context) =>
{
    var client = new OpenRouterClient(apiKey);
    
    // Register tools (if needed)
    client
        .RegisterTool<AddTool>()
        .RegisterTool<SubtractTool>();
    
    List<Message> messagesToSend;
    
    if (chatRequest.Messages != null && chatRequest.Messages.Count > 0)
    {
        // Client provides full history - server is STATELESS!
        messagesToSend = new List<Message>(chatRequest.Messages);
        messagesToSend.Add(Message.FromUser(chatRequest.Message));
    }
    else
    {
        // First message in conversation
        messagesToSend = new List<Message>
        {
            Message.FromSystem("You are a helpful assistant."),
            Message.FromUser(chatRequest.Message)
        };
    }
    
    var request = new ChatCompletionRequest
    {
        Model = chatRequest.Model ?? "anthropic/claude-3.5-sonnet",
        Messages = messagesToSend
    };
    
    // Enable artifacts (optional)
    request.EnableArtifactSupport(
        customInstructions: "Create artifacts for code examples."
    );
    
    // Stream response - NO server-side storage!
    await client.StreamAsSseAsync(request, context.Response);
    
    // ⚠️ CRITICAL: Do NOT save messages to any server-side store
    // Client handles all persistence
});

record ChatRequest(string Message, string? Model = null)
{
    public List<Message>? Messages { get; init; }
}
---

REACT/NEXT.JS FRONTEND PATTERN:
---
import { 
  useOpenRouterChat, 
  saveHistory, 
  loadHistory,
  clearHistory 
} from '@openrouter-dotnet/react';
import { useState, useEffect } from 'react';

function StatelessChat() {
  const [conversationId] = useState(`conv_${Date.now()}`);
  
  const { state, actions } = useOpenRouterChat({
    endpoints: { stream: '/api/stream-stateless' },
    defaultModel: 'anthropic/claude-3.5-sonnet'
  });
  
  // Load history when conversation changes
  useEffect(() => {
    const savedHistory = loadHistory(conversationId);
    actions.setMessages(savedHistory); // Populate hook state
  }, [conversationId]);
  
  // Auto-save after each message
  useEffect(() => {
    if (state.messages.length > 0) {
      saveHistory(conversationId, state.messages, {
        maxMessages: 100  // Auto-prune old messages
      });
    }
  }, [state.messages, conversationId]);
  
  const handleSend = async (input: string) => {
    // Send with full history from hook's state (synced with localStorage)
    await actions.sendMessage(input, {
      history: state.messages
    });
  };
  
  return (
    <div>
      <MessageList messages={state.messages} />
      <ChatInput onSend={handleSend} />
    </div>
  );
}
---

HOW IT WORKS:
1. Client loads conversation from localStorage
2. Client sends full history + new message to backend
3. Backend processes request (stateless - no storage)
4. Backend streams response back
5. Client receives response and saves to localStorage
6. Tool calls are automatically included in history

REACT SDK UTILITIES:

saveHistory(conversationId, messages, options?)
- Save messages to localStorage
- Options: maxMessages (auto-prune), keyPrefix (custom prefix)

loadHistory(conversationId, options?)
- Load messages from localStorage
- Returns ChatMessage[]
- Returns empty array if not found

clearHistory(conversationId, options?)
- Remove conversation from localStorage

listConversations(options?)
- Get list of all conversation IDs
- Returns string[]

getStorageSize(options?)
- Get total storage used in bytes
- Returns number

TOOL CALLS IN CLIENT-SIDE HISTORY:

Tool calls are automatically converted to backend format:

ChatMessage (React format):
{
  role: "assistant",
  blocks: [
    { type: "text", content: "I'll add those numbers" },
    { type: "tool_call", toolName: "add", status: "completed", result: "42" }
  ]
}

↓ Converted to ↓

Backend Message format:
[
  {
    role: "assistant",
    content: "I'll add those numbers",
    tool_calls: [{ id: "...", type: "function", function: { name: "add", arguments: "..." } }]
  },
  {
    role: "tool",
    tool_call_id: "...",
    content: "42"
  }
]

CRITICAL: Tool result messages are automatically created!

ARTIFACTS IN CLIENT-SIDE HISTORY:

Artifacts are included as text in message content:

ChatMessage with artifact:
{
  role: "assistant",
  blocks: [
    { type: "text", content: "Here's a React component:" },
    { type: "artifact", title: "Widget.tsx", content: "..." }
  ]
}

↓ Converted to ↓

{
  role: "assistant",
  content: "Here's a React component:\n\n[Artifact: Widget.tsx]\n..."
}

STATELESS VS SERVER-SIDE COMPARISON:

Server-Side History (Traditional):
❌ Memory grows unbounded
❌ Lost on restart
❌ Requires sticky sessions
❌ Single point of failure
✅ Simple implementation

Client-Side History (Production):
✅ Zero memory usage
✅ Survives restarts
✅ Infinite scale
✅ No session affinity
⚠️ Requires client management

BEST PRACTICES:

1. Set maxMessages to prevent quota issues:
   saveHistory(id, msgs, { maxMessages: 100 })

2. Use separate conversations for different topics

3. Clear old conversations periodically:
   const convs = listConversations();
   convs.forEach(id => clearHistory(id));

4. Monitor storage size:
   const size = getStorageSize();
   if (size > 5_000_000) { /* cleanup */ }

5. Handle tool calls transparently - SDK converts automatically

6. Enable artifacts in backend if needed

7. Don't send history on every message - only what's needed

COMPLETE WORKING EXAMPLE:

Backend (Program.cs):
---
app.MapPost("/api/stream-stateless", async (ChatRequest req, HttpContext ctx) =>
{
    var client = new OpenRouterClient(apiKey);
    client.RegisterTool<CalculatorTool>();
    
    var messages = req.Messages ?? new List<Message>
    {
        Message.FromSystem("You are a helpful assistant.")
    };
    messages.Add(Message.FromUser(req.Message));
    
    var request = new ChatCompletionRequest
    {
        Model = req.Model ?? "anthropic/claude-3.5-sonnet",
        Messages = messages
    };
    
    request.EnableArtifactSupport();
    await client.StreamAsSseAsync(request, ctx.Response);
});

record ChatRequest(string Message, string? Model = null)
{
    public List<Message>? Messages { get; init; }
}
---

Frontend (React):
---
function Chat() {
  const [id] = useState(`conv_${Date.now()}`);
  const { state, actions } = useOpenRouterChat({
    endpoints: { stream: '/api/stream-stateless' }
  });
  
  useEffect(() => {
    actions.setMessages(loadHistory(id));
  }, [id]);
  
  useEffect(() => {
    if (state.messages.length > 0) {
      saveHistory(id, state.messages);
    }
  }, [state.messages]);
  
  const send = async (text: string) => {
    await actions.sendMessage(text, {
      history: loadHistory(id)
    });
  };
  
  return <MessageList messages={state.messages} />;
}
---

This pattern is production-ready and handles tools, artifacts, and multi-turn
conversations with zero server memory usage.

See samples/NextJsClientSample/STATELESS_CHAT_README.md for complete guide.

================================================================================
7. ADVANCED FEATURES
================================================================================

7.1 MULTIMODAL (TEXT + IMAGES)
-------------------------------

Including images in user messages:

---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message>
    {
        Message.FromUser(new List<ContentPart>
        {
            new TextContent("What's shown in this image?"),
            new ImageContent("https://example.com/image.jpg")
        })
    }
};

var response = await client.CreateChatCompletionAsync(request);
---

Image detail control:

---
new ImageContent("https://example.com/image.jpg", detail: "high")
// detail: "auto" (default), "low", "high"
---

Base64 encoded images:

---
var base64Image = Convert.ToBase64String(File.ReadAllBytes("image.jpg"));
new ImageContent($"data:image/jpeg;base64,{base64Image}")
---

Multiple images:

---
Message.FromUser(new List<ContentPart>
{
    new TextContent("Compare these images:"),
    new ImageContent("https://example.com/image1.jpg"),
    new ImageContent("https://example.com/image2.jpg"),
    new TextContent("What's the key difference?")
})
---

Important notes:
- Not all models support image inputs - check model capabilities
- Image URLs must be publicly accessible (no auth required)
- Detail level affects token usage (high uses more tokens)
- Some models have image size limitations

7.2 JSON MODE
--------------

Ensure JSON output:

---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message>
    {
        Message.FromSystem("Return valid JSON only."),
        Message.FromUser("Generate a user profile object")
    },
    ResponseFormat = new ResponseFormat { Type = "json_object" }
};

var response = await client.CreateChatCompletionAsync(request);
var jsonResponse = response.GetContent();  // Guaranteed to be valid JSON
---

Best practices:
- Always include instructions in system message about JSON structure
- Specify fields and types expected
- Model may add extra fields but structure will be JSON
- Not all models support JSON mode

7.3 SYSTEM PROMPTS
-------------------

Basic system prompt:

---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message>
    {
        Message.FromSystem("You are a Python programming expert. Provide concise, well-documented code examples."),
        Message.FromUser("How do I read a file?")
    }
};
---

Dynamic system prompts:

---
string GetSystemPrompt(string role)
{
    return role switch
    {
        "python" => "You are an expert Python programmer. Provide Pythonic code.",
        "javascript" => "You are an expert JavaScript developer. Use modern ES6+ syntax.",
        "teaching" => "You are an excellent teacher. Explain concepts clearly.",
        _ => "You are a helpful assistant."
    };
}

var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message>
    {
        Message.FromSystem(GetSystemPrompt("python")),
        Message.FromUser("Write a function to sort a list")
    }
};
---

System prompt impact:
- Goes in Messages list as first message with role="system"
- Influences entire conversation
- More specific/detailed prompts give better results
- Can be updated in multi-turn conversations (not recommended)

7.4 PROVIDER AND MODEL SELECTION
----------------------------------

Specific provider:

---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Messages = new List<Message> { Message.FromUser("Hello") }
};
---

Model fallback/load balancing:

---
var request = new ChatCompletionRequest
{
    Models = new List<string>
    {
        "openai/gpt-4o",
        "anthropic/claude-3.5-sonnet",
        "meta-llama/llama-3.1-70b-instruct"
    },
    Messages = new List<Message> { Message.FromUser("Hello") }
};
// OpenRouter tries models in order, uses first available
---

Provider routing:

---
var request = new ChatCompletionRequest
{
    Model = "gpt-4o",  // Model name without provider
    Provider = new { name = "openai" },
    Messages = new List<Message> { Message.FromUser("Hello") }
};
---

Route preferences:

---
var request = new ChatCompletionRequest
{
    Model = "openai/gpt-4o",
    Route = "fallback",  // or other routing strategy
    Messages = new List<Message> { Message.FromUser("Hello") }
};
---

Available models:

---
var models = await client.GetModelsAsync();

var gptModels = models
    .Where(m => m.Id.StartsWith("openai/"))
    .OrderBy(m => m.Id)
    .ToList();

foreach (var model in gptModels)
{
    Console.WriteLine($"{model.Id}: {model.Name}");
    Console.WriteLine($"  Context: {model.ContextLength} tokens");
}
---

7.5 RATE LIMITING AND RETRIES
------------------------------

Handle rate limits:

---
var maxRetries = 3;
var retryDelay = TimeSpan.FromSeconds(1);

for (int attempt = 0; attempt < maxRetries; attempt++)
{
    try
    {
        var response = await client.CreateChatCompletionAsync(request);
        return response;
    }
    catch (OpenRouterRateLimitException ex)
    {
        if (attempt < maxRetries - 1)
        {
            var waitTime = ex.RetryAfterSeconds ?? (int)retryDelay.TotalSeconds;
            Console.WriteLine($"Rate limited. Waiting {waitTime}s...");
            await Task.Delay(TimeSpan.FromSeconds(waitTime));
            retryDelay = retryDelay.Add(TimeSpan.FromSeconds(2));  // Exponential backoff
        }
        else
        {
            throw;
        }
    }
}
---

Exponential backoff:

---
async Task<ChatCompletionResponse> SendWithRetryAsync(
    ChatCompletionRequest request,
    int maxAttempts = 5)
{
    var delay = TimeSpan.FromMilliseconds(100);
    
    for (int attempt = 0; attempt < maxAttempts; attempt++)
    {
        try
        {
            return await client.CreateChatCompletionAsync(request);
        }
        catch (OpenRouterRateLimitException ex) when (attempt < maxAttempts - 1)
        {
            var waitTime = ex.RetryAfterSeconds ?? (int)delay.TotalSeconds;
            await Task.Delay(TimeSpan.FromSeconds(waitTime));
            delay = delay.Add(delay);  // Double wait time
        }
    }
    
    throw new InvalidOperationException("Max retries exceeded");
}
---

Account limits:

---
var limits = await client.GetLimitsAsync();
Console.WriteLine($"Remaining requests: {limits.RemainingRequests}");
Console.WriteLine($"Rate limit: {limits.RateLimitPerMinute} requests/min");
---

================================================================================
8. COMMON PATTERNS FROM SAMPLES
================================================================================

8.1 INTERACTIVE MODEL SELECTION
--------------------------------

From BasicCliSample:

---
var models = await client.GetModelsAsync();

var popularModels = new[] { "openai/gpt-4o", "anthropic/claude-3.5-sonnet" };

var modelChoices = models
    .Select(m => m.Id)
    .OrderBy(m =>
    {
        var popularIndex = Array.IndexOf(popularModels, m);
        return popularIndex != -1 ? popularIndex : 100;
    })
    .ToList();

// Use with Spectre.Console for interactive selection:
var selectedModel = AnsiConsole.Prompt(
    new SelectionPrompt<string>()
        .Title("Select a model:")
        .PageSize(10)
        .EnableSearch()
        .AddChoices(modelChoices));
---

8.2 STREAMING WITH LIVE UPDATES
---------------------------------

Using Spectre.Console for progress:

---
var responseText = new StringBuilder();
TimeSpan? ttft = null;

await AnsiConsole.Live(CreateDisplay(responseText.ToString(), null))
    .StartAsync(async ctx =>
    {
        await foreach (var chunk in client.StreamAsync(request))
        {
            if (chunk.IsFirstChunk && chunk.TextDelta != null)
            {
                ttft = chunk.ElapsedTime;
            }
            
            if (chunk.TextDelta != null)
            {
                responseText.Append(chunk.TextDelta);
                ctx.UpdateTarget(CreateDisplay(responseText.ToString(), ttft));
            }
        }
    });
---

Display helper function:

---
static Table CreateDisplay(string content, TimeSpan? ttft)
{
    var table = new Table().Border(TableBorder.None);
    table.AddColumn("Response");
    
    if (ttft.HasValue)
    {
        table.AddRow($"[green]● Streaming[/] ([dim]{ttft.Value.TotalMilliseconds:F0}ms TTFT[/])");
    }
    else
    {
        table.AddRow("[yellow]● Waiting...[/]");
    }
    
    table.AddRow(content);
    return table;
}
---

8.3 BATCH PROCESSING
---------------------

Processing multiple requests:

---
var prompts = new[]
{
    "What is AI?",
    "Explain machine learning",
    "What is deep learning?"
};

var results = new List<string>();

foreach (var prompt in prompts)
{
    var request = new ChatCompletionRequest
    {
        Model = "openai/gpt-4o",
        Messages = new List<Message> { Message.FromUser(prompt) }
    };
    
    var response = await client.CreateChatCompletionAsync(request);
    results.Add(response.GetContent() ?? "");
}
---

Parallel batch processing:

---
var batchTasks = prompts.Select(async prompt =>
{
    var request = new ChatCompletionRequest
    {
        Model = "openai/gpt-4o",
        Messages = new List<Message> { Message.FromUser(prompt) }
    };
    
    var response = await client.CreateChatCompletionAsync(request);
    return (prompt, response: response.GetContent() ?? "");
});

var results = await Task.WhenAll(batchTasks);

foreach (var (prompt, response) in results)
{
    Console.WriteLine($"Q: {prompt}");
    Console.WriteLine($"A: {response}\n");
}
---

================================================================================
DIAGNOSTIC AND TESTING PATTERNS
================================================================================

From DiagnosticSample - comprehensive testing:

Streaming handler pattern:

---
public class StreamingHandler
{
    public async Task StreamResponseAsync(
        OpenRouterClient client,
        ChatCompletionRequest request,
        Action<(int chunkCount, TimeSpan ttft)> displayCallback)
    {
        int chunkCount = 0;
        TimeSpan? ttft = null;
        var artifacts = new List<Artifact>();
        
        try
        {
            await foreach (var chunk in client.StreamAsync(request))
            {
                chunkCount++;
                
                if (chunk.IsFirstChunk && chunk.TextDelta != null)
                {
                    ttft = chunk.ElapsedTime;
                }
                
                // Handle artifacts
                if (chunk.Artifact is ArtifactCompleted completed)
                {
                    artifacts.Add(new Artifact
                    {
                        Id = completed.ArtifactId,
                        Type = completed.Type,
                        Title = completed.Title,
                        Content = completed.Content
                    });
                }
                
                displayCallback((chunkCount, ttft ?? TimeSpan.Zero));
            }
        }
        catch (Exception ex)
        {
            // Log error
        }
    }
}
---

================================================================================
IMPORTANT NOTES AND BEST PRACTICES
================================================================================

1. API Key Security:
   - Never hardcode API keys
   - Use environment variables: Environment.GetEnvironmentVariable("OPENROUTER_API_KEY")
   - Use .NET user secrets in development
   - Rotate keys regularly

2. Streaming vs Non-Streaming:
   - Use streaming for long responses or real-time feedback
   - Use non-streaming for simple queries or when you need full response first
   - Streaming is more responsive and better for TTFT tracking
   - Both return same content, streaming provides it incrementally

3. Tool Calling:
   - Always register tools before streaming with tools
   - Tools are auto-included in requests if registered with client
   - Attribute-based registration preferred over manual
   - Handle errors gracefully - model may call non-existent tools
   - Set maxToolCalls to prevent infinite loops

4. Token Management:
   - Estimate tokens before sending large requests
   - Clear conversation history periodically in long sessions
   - Monitor token usage from responses (Usage field)
   - Different models have different tokenization

5. Error Handling:
   - Catch specific exception types: OpenRouterRateLimitException, OpenRouterAuthException
   - Implement exponential backoff for retries
   - Check RetryAfterSeconds for rate limits
   - Don't retry on authentication errors immediately

6. Performance:
   - Reuse client instance for multiple requests
   - Use connection pooling (HttpClient manages this)
   - Cache model list if listing models frequently
   - Monitor TTFT for performance analysis

7. Conversation Management:
   - Keep system message consistent across turns
   - Don't include the entire history in every request if it grows large
   - Use message roles correctly: system, user, assistant, tool
   - Clear history when changing topics

8. Model Selection:
   - Check model availability with GetModelsAsync()
   - Use fallback model lists for reliability
   - Consider context length for your use case
   - Some models are region/availability specific

9. Artifacts:
   - Enable artifact support for code generation tasks
   - Artifacts are parsed automatically during streaming
   - Collect complete artifacts in ArtifactCompleted events
   - Text outside artifacts is normal conversation text

10. Logging and Debugging:
    - Use OnLogMessage callback in OpenRouterClientOptions
    - Log StreamChunk data for debugging streaming
    - Save requests/responses to disk for reproducibility
    - Use diagnostic sample patterns for testing

================================================================================
END OF DOCUMENTATION
================================================================================

For latest information, see:
- GitHub: https://github.com/WilliamAvHolmberg/OpenRouter.NET
- OpenRouter API: https://openrouter.ai
- Documentation: Check README.md in project root

Last Updated: 2025
